---
title: "Reproducable Research and Figures Assignment"
output: html_document
date: "2024-12-11"
---

## Preparatory Steps

Loading libraries

```{r, message=FALSE}
library(tidyverse)
library(palmerpenguins)
library(here)
library(janitor)
library(ggplot2)
library(ragg)
library(svglite)
library(tinytex)
library(yaml)
```

Loading the data:

```{r}
head(penguins_raw)
colnames(penguins_raw)
write.csv(penguins_raw, here("data", "penguins_raw.csv"))
```

Cleaning the data:

```{r}
penguins_clean <- penguins_raw %>%
select(-Comments) %>%
select(-starts_with("Delta")) %>%
clean_names() %>%
mutate(species = gsub(" .*", "", species))  # Shorten species names

colnames(penguins_clean)
```

Saving the clean data:

```{r}
write_csv(penguins_clean, here("data", "penguins_clean.csv"))
```

## QUESTION 01: Data Visualisation for Science Communication

### a) Figure that is correct but badly communicates the data:

```{r bad figure code, echo=FALSE}
# Subsetting the columns body mass and flipper length, removing NAs
penguins_data_for_plotting <- penguins_clean %>%
  select(body_mass_g, flipper_length_mm, species) %>%
  na.omit(body_mass_g, flipper_length_mm)

# Poor visualization of data using a scatterplot of body mass (g) and flipper length (mm):
penguins_bodymass_flippers <-ggplot(penguins_data_for_plotting, aes(x = body_mass_g, y = flipper_length_mm, color = species)) +
  geom_point(size = 5, alpha = 1) +  # Big, opaque points
  theme_minimal() +
  labs(
    x = "Body Mass (g)",
    y = "Flipper Length (mm)"
  ) +
  theme(
    legend.position = "none"  # No legend
  )
penguins_bodymass_flippers
```

### b) How do the design choices mislead the reader about the underlying data? (100-300 words)

A scatter plot can be a useful exploratory figure to convey the relationship between body mass and flipper length in the palmerpenguins dataset. However, there are some design choices that make this particular scatter plot misleading and prevent it from conveying important information contained in the dataset.

1.  The high geom_point size obscures patterns in the data by making points severely overlap.

2.  The alpha value of 1 makes the points completely opaque, once again obscuring patterns in the data by making overlapping points hard to discern.

3.  The random colors chosen for the species and a lack of a legend prevents the reader from knowing which penguin species is represented by which colors. This hinders the figure from communicating important information about species-specific trends in the relationship between body mass and flipper length.

With a smaller geom_point size, a smaller alpha value, and a proper legend with appropriate species-specific colors, this scatter plot would be very effective in conveying how flipper length changes with increasing body mass for the three penguin species Adelie (Pygoscelis adeliae), Chinstrap (Pygoscelis antarcticus), and Gentoo (Pygoscelis papua).

------------------------------------------------------------------------

## QUESTION 2: Data Pipeline

```{r, message=FALSE}
# Loading libraries
library(tidyverse)
library(palmerpenguins)
library(here)
library(janitor)
library(ggplot2)
library(ragg)
library(svglite)
library(tinytex)
library(yaml)
```

### Introduction

Species occupy distinct ecological niches in a community which promote coexistence. One of the key contributors to niche differences between sympatric species are diet and foraging habits, adaptations for which are often reflected in the beak morphology in birds.

This analysis utilizes data collected from three largely sympatric penguin species - Adelie (Pygoscelis adeliae), Chinstrap (Pygoscelis antarcticus), and Gentoo (Pygoscelis papua) -to investigate whether there is a significant difference in beak length between them. Differences in beak length can be linked to wider differences in the beak morphology and are likely to reflect differences in feeding ecology between species. Insights from this analysis can contribute towards an increased understanding of these species' ecologies, which is important when making predictions about their future habitat and survival.

The data for this analysis is contained in the palmerpenguins dataset. The dataset was first loaded into RStudio, then cleaned to ensure consistency in the names of rows and columns, shorten species names, and remove empty rows and columns.

```{r}
# Loading the data
library(palmerpenguins)
head(penguins_raw)
colnames(penguins_raw)
write.csv(penguins_raw, here("data", "penguins_raw.csv"))

# Cleaning the data
penguins_clean <- penguins_raw %>%
select(-Comments) %>%
select(-starts_with("Delta")) %>%
clean_names() %>%
mutate(species = gsub(" .*", "", species))  # Shorten species names

colnames(penguins_clean)
```

Following cleaning, the data was subsetted to only include culmen length, culmen depth (analagous to bill length and depth), and species information to explore bill dimensions in Adelie, Chinstrap, and Gentoo penguins.

```{r}
# Subsetting the columns culmen length, culmen depth, species and removing NAs
penguins_data_for_scatterplot <- penguins_clean %>%
  select(culmen_length_mm, culmen_depth_mm, species) %>%
  na.omit(culmen_length_mm, culmen_depth_mm)
```

### Hypothesis

There is a statistically significant difference in the mean beak lengths of Adelie, Chinstrap, and Gentoo penguins.

### Statistical Methods

This analysis used ANOVA to test for differences between the means of the three groups (species) and performed a post-hoc Tukey HSD test to investigate pairwise differences in mean beak lengths.

```{r}
# One-way ANOVA
anova_result <- aov(culmen_length_mm ~ species, data = penguins_data_for_scatterplot)

# Post-hoc Tukey HSD test for pairwise comparisons
tukey_result <- TukeyHSD(anova_result)
```

### Results & Discussion

A scatter plot of the raw data was created to explore bill dimensions in the three penguin species.

```{r}
# Define color mapping with names for each species
species_colours <- c(
  "Adelie" = "darkorange", 
  "Chinstrap" = "purple", 
  "Gentoo" = "cyan4")

# Scatter plot of bill dimensions by species with custom colors
bill_scatterplot <- ggplot(penguins_data_for_scatterplot, aes(x = culmen_length_mm, y = culmen_depth_mm, color = species)) +
  geom_point(alpha = 0.7, size = 3) +
  scale_color_manual(values = species_colours) +
  labs(
    title = "Bill Dimensions by Penguin Species",
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "Species"
  ) +
  theme_minimal()

# Display the plot
print(bill_scatterplot)
```

The scatter plot shows how beak depth changes in relation to beak length, and visual inspection of it reveals that the three species occupy distinct areas of the plot, hinting at differences in beak morphology between them. While Adelie penguins' beaks are shorter but deeper, those of Gentoo penguins seem longer yet less deep. Chinstraps occupy an area between these two species on the curve, with intermediate beak length and depth.

The figure was appropriately scaled and saved.

```{r}
# Saving as .png
agg_png("figures/bill_scatterplot.png", 
        width = 40, height = 40, units = "cm", res = 300, scaling = 2)
print(bill_scatterplot)
dev.off()

# Saving as .svg
inches_conversion = 2.54
svglite("figures/bill_scatterplot.svg", 
        width = 40 / inches_conversion, 
        height = 40 / inches_conversion, 
        scaling = 2)
print(bill_scatterplot)
dev.off()
```

Next, an ANOVA was conducted to test for differences in mean beak depths between the three penguin species, after which the Tukey HSD test was performed to investigate pairwise comparisons.

```{r}
summary(anova_result)
print(tukey_result)
```

The ANOVA results indicate that there is a statistically significant difference in bill length across the three penguin species. This is due to the highly significant p-value at the 0.001 level. This result is consistent with the hypothesis that penguin species have differing bill lengths, likely due to species-specific ecological adaptations.

All p-values from the Tukey test are \<0.05, indicating the significant difference in bill length in all pairwise species comparisons. Additionally, the 95% confidence intervals for the differences do not include 0 for any pair. Taken together, the results of the ANOVA and Tukey test suggest a statistically significant difference in the bill lengths between all three penguin species in the analysis, supporting the hypothesis. The differences in bill length likely reflect ecological adaptations of the penguin species to their respective diet and foraging strategies.

Following this, an interval plot was produced which visualizes the differences in mean beak length for all pairwise comparisons, including their relative statistical significance taken from the Tukey test.

Before producing the plot, results of the Tukey HSD test were extracted into a data frame, to which a species comparison column and indication of significance levels were added.

```{r}
# Extracting the Tukey HSD results into a dataframe:
tukey_df <- as.data.frame(tukey_result$species)

# Adding a column for the species comparisons
tukey_df$Comparison <- rownames(tukey_df)

# Creating the pairwise_diff data frame with significance levels indicated
pairwise_diff <- tukey_df %>%
  select(Comparison, diff, lwr, upr, `p adj`) %>%
  rename(
    Difference = diff,
    Lower = lwr,
    Upper = upr,
    p_value = `p adj`
  )  %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "***",
      p_value < 0.01 ~ "**",
      p_value < 0.05 ~ "*",
      TRUE ~ "ns"  # Not significant
    ) )

# Print the resulting pairwise_diff data frame
print(pairwise_diff)
```

```{r}
# Interval plot with significance annotations
interval_plot <- ggplot(pairwise_diff, aes(x = Comparison, y = Difference)) +
  geom_point(size = 4, color = "maroon2") +  # Mean differences as points
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, color = "black") + # Vertical error bars
  geom_hline(yintercept = 0, linetype = "dashed", color = "navyblue", linewidth = 0.8) +  # Zero difference reference line
  geom_text(
    aes(label = significance, y = Upper + 0.5),  # Positioning significance levels above the error bars
    size = 5,
    color = "black"
  ) +
  labs(title = "Pairwise Differences in Bill Length Between Penguin Species",
    x = "Species Comparison",
    y = "Difference in Mean Bill Length (mm)") +
  theme_bw()

# Display the plot
print(interval_plot)
```

The interval plot highlights the statistical significance of all pairwise comparisons and shows that the 95% confidence interval does not overlap 0 for any pair. From this figure, it can be concluded that the average beak length is highest for Chinstrap penguins followed by Gentoo. Adelie penguins have the shortest beaks between the three species.

The significant differences in the mean bill lengths of the three penguin species likely reflect ecological adaptations to different feeding niches. Adelie penguins feed largely on fish and krill, and usually catch their prey near the surface of the water. Like Adelie penguins, Chinstrap penguins also have a diet consisting of fish and krill (Trivelpiece et al., 1987). Both of these species feed offshore and are shallow divers, although Chinstraps tend to dive deeper than Adelie penguins. Gentoos have a more diverse generalist diet consisting of squid, fish, and crustaceans. They feed inshore and dive to depths of up to 200 m to catch their prey. Their ability to dive deeper enables them access to a feeding niche which is unavailable to the other two species (Trivelpiece et al., 1987). Although these differences in diet composition and foraging behaviour may seem minor, they can be enough to result in distinct niches for these species to ensure their coexistence. It is important to note that size differences exist between these species, however the size order is not mirrored in the order of beak length. Gentoos are the largest of these three species yet come second in average beak length. This finding strengthens the idea that differences in bill length in these species are partly a result of differences in feeding ecology, although more detailed geometric morphometric analyses paired with behavioural observations are needed for further support.

### Conclusion

This analysis aimed to test whether there is a statistically significant difference in bill length between Adelie, Chinstrap, and Gentoo penguins. The bill morphology of penguins is shaped by adaptations to their feeding ecology since bills are used in the acquisition and consumption of food items. The finding that the mean beak lengths differ significantly between these three penguin species hints at potential differences regarding their feeding niches. Additional factors to be considered in further analyses include beak length relative to body size, as well as other parameters characterizing beak morphology.

### References

-   A., L. et al. (2002) ‘Conflict or co-existence? foraging distribution and competition for prey between Adélie and chinstrap penguins.’, Marine Biology, 141(6), pp. 1165–1174. <doi:10.1007/s00227-002-0899-1>. 

-   Birds & Seals of the Falkland Islands- Falklands birds & seals - Falkland Islands wildlife guide. Available at: <http://www.seabirds.org/birds.htm> (Accessed: 11 December 2024). 

-   Oceanwide Expeditions. Available at: <https://oceanwide-expeditions.com/blog/meet-all-6-antarctic-penguin-species> (Accessed: 11 December 2024). 

-   Gentoo Penguins \~ Marinebio Conservation Society (2023) MarineBio Conservation Society. Available at: <https://www.marinebio.org/species/gentoo-penguins/pygoscelis-papua/> (Accessed: 11 December 2024). 

-   Trivelpiece, W.Z., Trivelpiece, S.G. and Volkman, N.J. (1987) ‘Ecological segregation of Adelie, Gentoo, and chinstrap penguins at king george island, Antarctica’, Ecology, 68(2), pp. 351–361. <doi:10.2307/1939266>. 

------------------------------------------------------------------------

## Question 3: Open Science

### a) GitHub

*Upload your RProject you created for **Question 2** and any files and subfolders used to GitHub. Do not include any identifiers such as your name. Make sure your GitHub repo is public.*

*GitHub link:*

<https://github.com/biouser2/Reproducable-Science-Assignment>

### b) Share your repo with a partner, download, and try to run their data pipeline.

*Partner's GitHub link:*

<https://github.com/Biology11/Reproducible-Science-and-Figures>

### c) Reflect on your experience running their code. (300-500 words)

Running my partner’s code was mostly a straightforward process. The analysis is structured clearly and well. The use of headings and comments made it easy to understand the function of each code chunk, even if the specifics of the code were not familiar to me. The necessary libraries were loaded and my partner’s code ran without needing to fix anything. The use of intuitive and descriptive variable names, such as “penguins_clean” and “adelie_penguins”, made the code easy to read for a human and not just for a computer. It would have been good to save datasets such as “penguins_clean” as .csv files for easy access in the future. The use of piping to clean the data contributed to the reproducibility of the code as well as making it easy to alter the code for different analysis needs in the future. The line of code where the column name “Sex” is modified to not be capitalized could have been included in the data cleaning pipe as well; this would overcome the confusion created by having variables called “adelie_penguins” as well as adelie_penguins\$sex.  Additionally, creating a function in R would have made these steps even more reproducible. If my partner were to create a “cleaning function” for example, it could be possible to use the same code for a different dataset, making their analysis replicable. 

The figures produced in the analysis are very clear in communicating the relevant information from the dataset. The transparency of the bins of the histogram are useful in showing the distribution for males and females in the area of overlap. For some of the less intuitive elements of the code, including a comment next to certain functions could improve clarity for users. For example, it could be useful to add a comment after the annotate(“text”,...) line when plotting the histogram to explain what this line of code does. Additionally, writing code to display the mean of each distribution on the figure as a dot, perhaps indicating the level of significance between the means with asterisks on the figure as well, would make the plot more efficient in conveying the relevant information. However even in this step, the code my partner wrote is reproducible since it does not directly include a number for the mean flipper length in the code and instead uses columns in the two sample t-test results table. One way of making the figures easier to manipulate for other users is by defining a color vector for both sexes beforehand and including the color vector in the code instead of manually assigning colors. Despite minor suggestions, the code and analysis runs without problem and is effective in communicating important information, as well as being largely reproducible.

### d) Reflect on your own code based on your experience with your partner's code and their review of yours. (300-500 words)

The exercise of trying to run my partner’s code and get their feedback on running mine was a very informative process from which I have gained valuable insights into writing reproducible code. An improvement my partner suggested to me was creating a function for the data cleaning and subsetting stages, which would improve the reproducibility of my code and could make it be used on different datasets seamlessly. This was very noteworthy to me as my main suggestion to my partner’s code was also the same: creating functions for steps such as data cleaning. It has now become clear to me that while one may think their line of code is well annotated and thus reproducible, creating functions where they are useful enables research to go an extra step farther in terms of reproducibility. Based on personal experience, as well as the literature on the barriers to reproducibility in science, I believe what prevents many people from writing their own functions in R is a lack of experience with such practices. However, considering all of the information available online which can aid one in writing their own functions, as well as their utility in making research more reproducible, I can say that this is something I will look to implement in my future coding projects. 

Another comment I received was that my partner had difficulty running my code to save figures, and that this part of my code should be more thoroughly annotated. I agree with this suggestion and would ensure annotations are present in all less intuitive elements of my code for the future. Based on my experience with my partner’s code, I can see that using annotations where necessary can be very useful in making code accessible and understandable even to people who are not familiar with the methods used in the analysis. Some positive feedbacks I got included the use of species-specific color vectors for figures. This was an area in which I suggested improvements to my partner’s code as it ensures consistency amongst the figures as well as aiding in reproducibility. Overall, this exercise has demonstrated the importance of using best practices when coding to ensure reproducibility, and elements which are important for this include writing functions where appropriate, annotating the code for elements which are less intuitive, writing intuitive code to save datasets and plots, as well as defining color vectors to be used in figures. 

### e) What are the main barriers for scientists to share their data and code, and what could be done to overcome them? (500-700 words)

\
Despite the demonstrated benefits of data and code sharing, as well as the importance of this practice for the advancement of science, its wider adoption is hindered by certain barriers. For instance, researchers may fear being “scooped”, where another researcher completes an analysis the author had intended to do themselves (Gomes et al., 2022). The time required to clean one’s code to ensure it is reproducible and understandable to others presents another barrier to the dissemination of code. Scientists may fear that if they make their data and code public, others may find errors in them which may hinder their career progress. Additionally, many researchers might simply not know best practices to ensure the reproducibility of their analyses, as well as how to make their data and code public (Gomes et al., 2022).

It is important to acknowledge that data and code sharing are not only beneficial for the advancement of science by reducing the time researchers spend rewriting code which could otherwise be obtained through sharing, but also for the individual scientist themselves. In fact, various studies have demonstrated the increased citation rate of papers as a result of sharing their data and code (McKiernan et al., 2016). It is important for researchers to consider that their rights to ownership of data or code can be protected by appropriate licensing, and that the opportunity to publish pre-prints allows them to disseminate their ideas rapidly and claim the projects they want to conduct (Gomes et al., 2022). To overcome fears of inadequate code, it is essential that researchers familiarize themselves with best practices in coding, which will not only allow them to engage in reproducible science, but also make it easier for themselves to rerun analyses in the future or do further work on a given project. One study aimed to establish the reproducibility of research uploaded onto the Harvard Database repository, and found that they were able to reduce the percentage of R files that produced an error when running from 74% to 56% by implementing a simple code cleaning procedure (Trisovic et al., 2022). Their analysis showed that the reproducibility of research may be increased through the use of literate programming, for example by adding comments to explain code chunks (Trisovic et al., 2022). It was noted, however, that best practices in this regard suggest minimizing the number of comments to those necessary, and implementing intuitive names for functions and variables remains essential for reproducibility. The use of formats such as RMarkdown allow ease of use of the code, which also improves reproducibility. The authors frequently observed errors due to the incompatibility of newer versions of R libraries with the initial analysis, and strongly recommend an increased adoption of the renv package in R which documents the libraries and versions used in an analysis (Trisovic et al., 2022). Although researchers might not be used to this practice, its wider adoption holds the potential to significantly improve the reproducibility of analyses. 

Lastly, responsibility falls on journals and funding bodies to incentivize data and code sharing. Although some funding institutions, like the NIH in the United States, have been mandating data sharing for decades, there is still the need for more organizations to adopt such policies (Gomes et al., 2022). Although some journals mandate or strongly encourage  the sharing of data and code, there is scope for significant improvement in this area, especially regarding the effectiveness of the implementation of these policies. Journals should provide links to the repository where the data and code in an analysis is stored, and could even employ code editors as part of the peer review process  (Maitner et al., 2023). As for individuals, the sharing of data and code as well as adoption of open science in general, is expected to provide benefits to journals owing to an increase in citation rates and impact.

### References

-   Maitner, B. *et al.* (2023) *Code sharing increases citations, but remains uncommon* [Preprint]. <doi:10.21203/rs.3.rs-3222221/v1>. 
-   Trisovic, A. *et al.* (2022) ‘A large-scale study on research code quality and execution’, *Scientific Data*, 9(1). <doi:10.1038/s41597-022-01143-6>. 
-   A Rock-Star Researcher Spun a Web of Lies—and Nearly Got Away with It. <https://thewalrus.ca/a-rock-star-researcher-spun-a-web-of-lies-and-nearly-got-away-with-it/>
-   Gomes, D.G. *et al.* (2022) ‘Why don’t we share data and code? perceived barriers and benefits to public archiving practices’, *Proceedings of the Royal Society B: Biological Sciences*, 289(1987). <doi:10.1098/rspb.2022.1113>. 
-   McKiernan, E.C. *et al.* (2016a) ‘How open science helps researchers succeed’, *eLife*, 5. <doi:10.7554/elife.16800>. 
-   Baker, M. (2016) ‘1,500 scientists lift the lid on reproducibility’, *Nature*, 533(7604), pp. 452–454. <doi:10.1038/533452a>. 
